{
 "metadata": {
  "name": "",
  "signature": "sha256:a626edda1059afa75d03eb1ca0980e77b0824d8f69b746a82f2c2db3816c3965"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "pyspark-pictures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#map\">\n",
      "<img align=left src=\"files/images/pyspark-page1.svg\" width=500 height=250 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.map(lambda x: (x,x**2))\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[(1, 1), (2, 4), (3, 9)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page2.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "print(x.flatMap(lambda x: (x, 100*x, x**2)).collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page3.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(iterator): yield sum(iterator)\n",
      "y = x.mapPartitions(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[1], [5]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page4.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
      "y = x.mapPartitionsWithIndex(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[(0, 1)], [(1, 5)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page5.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "print(x.getNumPartitions())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page6.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.filter(lambda x: x%2 == 1)  # filter even elements\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[1, 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page7.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A','A','B'])\n",
      "y = x.distinct()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'A', 'B']\n",
        "['A', 'B']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page8.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(10)]\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:1 y = [2, 3, 5]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:2 y = [0, 1, 2, 5]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:3 y = [0, 1, 4, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:4 y = [2, 5]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:5 y = [6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:6 y = [3, 4, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:7 y = [0, 1, 2, 3, 4, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:8 y = [0, 1, 2, 5]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:9 y = [0, 3, 4, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page9.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(10)]\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [0, 1, 4]\n",
        "sample:1 y = [6, 4, 3]\n",
        "sample:2 y = [3, 4, 1]\n",
        "sample:3 y = [3, 4, 1]\n",
        "sample:4 y = [5, 2, 6]\n",
        "sample:5 y = [0, 4, 1]\n",
        "sample:6 y = [2, 1, 5]\n",
        "sample:7 y = [4, 0, 6]\n",
        "sample:8 y = [3, 4, 0]\n",
        "sample:9 y = [6, 5, 3]\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page10.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = sc.parallelize([1,5,6])\n",
      "z = x.union(y)\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3, 1, 5, 6]\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page11.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A','A','B'])\n",
      "y = sc.parallelize(['A','C','D'])\n",
      "z = x.intersection(y)\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A']\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page12.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
      "y = x.sortByKey()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('A', 2), ('C', 3)]\n",
        "[('A', 2), ('B', 1), ('C', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page13.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([2,1,3])\n",
      "y = x.sortBy(lambda x: x**2)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 1, 3]\n",
        "[1, 2, 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page14.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page15.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([(\"a\", (1,1)), (\"a\", 2), (\"b\", (3,3)), (\"c\", 4)])\n",
      "y = sc.parallelize([(\"d\", (5,5)), (\"a\", 6), (\"b\", 7), (\"a\", 8)])\n",
      "z = x.cogroup(y).collect()\n",
      "print(z)\n",
      "print(z[0])\n",
      "print(z[1])\n",
      "print(z[0][0])\n",
      "print(z[0][1])\n",
      "\n",
      "# z[which key?][key or value?][x or y?].data\n",
      "\n",
      "for el in z:\n",
      "    key = el[0]\n",
      "    xval = el[1][0]\n",
      "    yval = el[1][1]\n",
      "    print(\"key: \" + str(key))\n",
      "    print(\"x vals: \" + str(xval.data))\n",
      "    print(\"y vals: \" + str(yval.data))\n",
      "    \n",
      "ztest = x.cogroup(y)\n",
      "print(\"debug string: \" + ztest.toDebugString())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('a', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb390>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb3d0>)), ('c', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb410>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb450>)), ('b', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb490>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb4d0>)), ('d', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb510>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb550>))]\n",
        "('a', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb390>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb3d0>))\n",
        "('c', (<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb410>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb450>))\n",
        "a\n",
        "(<pyspark.resultiterable.ResultIterable object at 0x7f9b155eb390>, <pyspark.resultiterable.ResultIterable object at 0x7f9b155eb3d0>)\n",
        "key: a\n",
        "x vals: [(1, 1), 2]\n",
        "y vals: [6, 8]\n",
        "key: c\n",
        "x vals: [4]\n",
        "y vals: []\n",
        "key: b\n",
        "x vals: [(3, 3)]\n",
        "y vals: [7]\n",
        "key: d\n",
        "x vals: []\n",
        "y vals: [(5, 5)]\n",
        "debug string: (8) PythonRDD[21] at RDD at PythonRDD.scala:43 []\n",
        " |  MappedRDD[20] at values at NativeMethodAccessorImpl.java:-2 []\n",
        " |  ShuffledRDD[19] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
        " +-(8) PairwiseRDD[18] at cogroup at <ipython-input-3-e5dd5a652725>:20 []\n",
        "    |  PythonRDD[17] at cogroup at <ipython-input-3-e5dd5a652725>:20 []\n",
        "    |  UnionRDD[16] at union at NativeMethodAccessorImpl.java:-2 []\n",
        "    |  PythonRDD[14] at RDD at PythonRDD.scala:43 []\n",
        "    |  ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:364 []\n",
        "    |  PythonRDD[15] at RDD at PythonRDD.scala:43 []\n",
        "    |  ParallelCollectionRDD[5] at parallelize at PythonRDD.scala:364 []"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([(\"a\", (1,1)), (\"a\", 2), (\"b\", (3,3)), (\"c\", 4)])\n",
      "y = sc.parallelize([(\"d\", (5,5)), (\"a\", (3,3)), (\"b\", (3,3)), (\"a\", 8)])\n",
      "print(x.subtract(y).collect())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('a', 2), ('c', 4), ('a', (1, 1))]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(10))\n",
      "print(x.collect())\n",
      "print(x.keyBy(lambda x: x**2).collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "[(0, 0), (1, 1), (4, 2), (9, 3), (16, 4), (25, 5), (36, 6), (49, 7), (64, 8), (81, 9)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}