{
 "metadata": {
  "name": "",
  "signature": "sha256:cc7ba321ffdcd936adc13e3c339347e4d8d494eaaa1c99871b1548062d1b67e4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "pyspark-pictures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#map\">\n",
      "<img align=left src=\"files/images/pyspark-page1.svg\" width=500 height=250 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.map(lambda x: (x,x**2))\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[(1, 1), (2, 4), (3, 9)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page2.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "print(x.flatMap(lambda x: (x, 100*x, x**2)).collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page3.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(iterator): yield sum(iterator)\n",
      "y = x.mapPartitions(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[1], [5]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page4.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
      "y = x.mapPartitionsWithIndex(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[(0, 1)], [(1, 5)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page5.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3], 2)\n",
      "print(x.getNumPartitions())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page6.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.filter(lambda x: x%2 == 1)  # filter even elements\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[1, 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page7.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A','A','B'])\n",
      "y = x.distinct()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'A', 'B']\n",
        "['A', 'B']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page8.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)]\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [1, 2, 3, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:1 y = [0, 2, 3, 4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:2 y = [0, 1, 2, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:3 y = [0, 5]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:4 y = [3, 4, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page9.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [5, 1, 2]\n",
        "sample:1 y = [6, 0, 4]\n",
        "sample:2 y = [3, 5, 4]\n",
        "sample:3 y = [3, 2, 0]\n",
        "sample:4 y = [3, 0, 1]\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page10.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = sc.parallelize([1,5,6])\n",
      "z = x.union(y)\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3, 1, 5, 6]\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page11.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A','A','B'])\n",
      "y = sc.parallelize(['A','C','D'])\n",
      "z = x.intersection(y)\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A']\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page12.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
      "y = x.sortByKey()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('A', 2), ('C', 3)]\n",
        "[('A', 2), ('B', 1), ('C', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page13.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([2,1,3])\n",
      "y = x.sortBy(lambda x: x**2)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 1, 3]\n",
        "[1, 2, 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page14.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['C','B','A'], 2)\n",
      "y = x.glom()\n",
      "print(x.collect()) \n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['C', 'B', 'A']\n",
        "[['C'], ['B', 'A']]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page15.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A','B'])\n",
      "y = sc.parallelize(['C','D'])\n",
      "z = x.cartesian(y)\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page16.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.groupBy(lambda x: 'A' if (x%2 == 1) else 'B' )\n",
      "print(x.collect())\n",
      "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[('A', [1, 3]), ('B', [2])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page17.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(['A', 'Ba', 'C', 'AD'])\n",
      "y = x.pipe('grep -i \"A\"')\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'Ba', 'C', 'AD']\n",
        "['A', 'Ba', 'AD']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page18.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "def f(el):\n",
      "    '''side effect: append the current RDD element into a file'''\n",
      "    f1=open(\"./foreachExample.txt\", 'a+') \n",
      "    print(el,file=f1)\n",
      "\n",
      "open('./foreachExample.txt', 'w').close()  # clear the file contents\n",
      "y = x.foreach(f)\n",
      "\n",
      "print(x.collect())\n",
      "print(y)\n",
      "# print the contents of foreachExample.txt\n",
      "with open(\"./foreachExample.txt\", \"r\") as foreachExample:\n",
      "    print (foreachExample.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "None\n",
        "2\n",
        "3\n",
        "1\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 208
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page19.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3],5)\n",
      "def f(parition):\n",
      "    '''side effect: append the current RDD partition contents into a file'''\n",
      "    f1=open(\"./foreachPartitionExample.txt\", 'a+') \n",
      "    print([el for el in parition],file=f1)\n",
      "\n",
      "open('./foreachPartitionExample.txt', 'w').close()  # clear the file contents\n",
      "y = x.foreachPartition(f)\n",
      "\n",
      "print(x.glom().collect())\n",
      "print(y)\n",
      "# print the contents of foreachExample.txt\n",
      "with open(\"./foreachPartitionExample.txt\", \"r\") as foreachExample:\n",
      "    print (foreachExample.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[], [1], [], [2], [3]]\n",
        "None\n",
        "[]\n",
        "[1]\n",
        "[2]\n",
        "[]\n",
        "[3]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page20.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.collect()\n",
      "print(x)\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ParallelCollectionRDD[950] at parallelize at PythonRDD.scala:364\n",
        "[1, 2, 3]\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page21.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "y = x.reduce(lambda obj, accumulated: accumulated + obj)  # computes cumulative sum\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 233
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page22.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,2,3])\n",
      "neutral_zero_value = 0\n",
      "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 227
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page23.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
      "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
      "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
      "x = sc.parallelize([2,3,4])\n",
      "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 3, 4]\n",
        "(9, 24)\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page24.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.max()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "3\n"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page25.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.min()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page26.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sum()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page27.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.count()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "3\n"
       ]
      }
     ],
     "prompt_number": 243
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page28.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.histogram(buckets = 2)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "([1, 2, 3], [2, 3])\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page29.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.mean()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "2.0\n"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page30.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.variance()  # divides by N\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "0.666666666667\n"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page31.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.stdev()  # divides by N\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "0.816496580928\n"
       ]
      }
     ],
     "prompt_number": 247
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page32.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sampleStdev() # divides by N-1\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page33.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sampleVariance()  # divides by N-1\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 249
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page34.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.countByValue()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "defaultdict(<type 'int'>, {1: 2, 2: 1, 3: 2})\n"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page35.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.top(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[3, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 251
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page36.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.takeOrdered(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[1, 1, 2]\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page37.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.take(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[1, 3, 1]\n"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page38.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.first()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 254
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page39.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.collectAsMap()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "{'A': 1, 'C': 3, 'B': 2}\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page40.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.keys()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "['C', 'A', 'B']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page41.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.values()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "[3, 1, 2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page42.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', 12), ('B', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 260
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page43.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.reduceByKeyLocally(lambda agg, obj: agg + obj)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "{'A': 12, 'B': 3}\n"
       ]
      }
     ],
     "prompt_number": 263
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page44.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.countByKey()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "defaultdict(<type 'int'>, {'A': 3, 'B': 2})\n"
       ]
      }
     ],
     "prompt_number": 264
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page45.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.join(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('B', (3, 7))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 266
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page46.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.leftOuterJoin(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('C', (4, None)), ('B', (3, 7))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page47.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.rightOuterJoin(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('B', (3, 7)), ('D', (None, 5))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 268
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page48.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([(0,1),(1,2),(2,3)],2)\n",
      "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc\n",
      "print(x.glom().collect())\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(0, 1)], [(1, 2), (2, 3)]]\n",
        "[[(0, 1)], [(1, 2)], [(2, 3)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 287
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page49.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createCombiner = (lambda el: [(el,el**2)]) \n",
      "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
      "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page50.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zeroValue = [] \n",
      "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
      "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page51.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zeroValue = 1 \n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', 60), ('B', 2)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 296
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page52.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
      "y = x.groupByKey()\n",
      "print(x.collect())\n",
      "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
        "[('A', [3, 2, 1]), ('B', [5, 4])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 298
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page53.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
      "y = x.flatMapValues(lambda x: x)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
        "[('A', 1), ('A', 2), ('A', 3), ('B', 4), ('B', 5)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 300
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page54.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
      "y = x.mapValues(lambda x: x[-1]) # get the last value in the tuple\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
        "[('A', 3), ('B', 5)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 305
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page55.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
      "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
      "z = sc.parallelize([('D',9),('B',(8,8))])\n",
      "result = x.groupWith(y,z)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())\n",
      "localResult = list(result.collect())\n",
      "for key,val in localResult:\n",
      "    print(key, [list(i) for i in val])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
        "[('B', (7, 7)), ('A', 6), ('D', (5, 5))]\n",
        "[('D', 9), ('B', (8, 8))]\n",
        "D"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[], [(5, 5)], [9]]\n",
        "C [[4], [], []]\n",
        "B [[(3, 3)], [(7, 7)], [(8, 8)]]\n",
        "A [[2, (1, 1)], [6], []]\n"
       ]
      }
     ],
     "prompt_number": 340
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page56.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
      "result = x.groupWith(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "localResult = list(result.collect())\n",
      "for key,val in localResult:\n",
      "    print(key, [list(i) for i in val])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', (5, 5))]\n",
        "A"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[2, (1, 1)], [8, 6]]\n",
        "C [[4], []]\n",
        "B [[(3, 3)], [7]]\n",
        "D [[], [(5, 5)]]\n"
       ]
      }
     ],
     "prompt_number": 342
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page57.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page58.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page59.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page60.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page61.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page62.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page63.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page64.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"https://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page65.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([(\"a\", (1,1)), (\"a\", 2), (\"b\", (3,3)), (\"c\", 4)])\n",
      "y = sc.parallelize([(\"d\", (5,5)), (\"a\", 6), (\"b\", 7), (\"a\", 8)])\n",
      "z = x.cogroup(y).collect()\n",
      "print(z)\n",
      "print(z[0])\n",
      "print(z[1])\n",
      "print(z[0][0])\n",
      "print(z[0][1])\n",
      "\n",
      "# z[which key?][key or value?][x or y?].data\n",
      "\n",
      "for el in z:\n",
      "    key = el[0]\n",
      "    xval = el[1][0]\n",
      "    yval = el[1][1]\n",
      "    print(\"key: \" + str(key))\n",
      "    print(\"x vals: \" + str(xval.data))\n",
      "    print(\"y vals: \" + str(yval.data))\n",
      "    \n",
      "ztest = x.cogroup(y)\n",
      "print(\"debug string: \" + ztest.toDebugString())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('a', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbcb90>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbc910>)), ('c', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbc850>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbcf50>)), ('b', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbc4d0>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbcd50>)), ('d', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbca50>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbc550>))]\n",
        "('a', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbcb90>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbc910>))\n",
        "('c', (<pyspark.resultiterable.ResultIterable object at 0x7f939cbbc850>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbcf50>))\n",
        "a\n",
        "(<pyspark.resultiterable.ResultIterable object at 0x7f939cbbcb90>, <pyspark.resultiterable.ResultIterable object at 0x7f939cbbc910>)\n",
        "key: a\n",
        "x vals: [(1, 1), 2]\n",
        "y vals: [6, 8]\n",
        "key: c\n",
        "x vals: [4]\n",
        "y vals: []\n",
        "key: b\n",
        "x vals: [(3, 3)]\n",
        "y vals: [7]\n",
        "key: d\n",
        "x vals: []\n",
        "y vals: [(5, 5)]\n",
        "debug string: (8) PythonRDD[628] at RDD at PythonRDD.scala:43 []\n",
        " |  MappedRDD[627] at values at null:-1 []\n",
        " |  ShuffledRDD[626] at partitionBy at null:-1 []\n",
        " +-(8) PairwiseRDD[625] at cogroup at <ipython-input-113-e5dd5a652725>:20 []\n",
        "    |  PythonRDD[624] at cogroup at <ipython-input-113-e5dd5a652725>:20 []\n",
        "    |  UnionRDD[623] at union at null:-1 []\n",
        "    |  PythonRDD[621] at RDD at PythonRDD.scala:43 []\n",
        "    |  ParallelCollectionRDD[611] at parallelize at PythonRDD.scala:364 []\n",
        "    |  PythonRDD[622] at RDD at PythonRDD.scala:43 []\n",
        "    |  ParallelCollectionRDD[612] at parallelize at PythonRDD.scala:364 []\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize([(\"a\", (1,1)), (\"a\", 2), (\"b\", (3,3)), (\"c\", 4)])\n",
      "y = sc.parallelize([(\"d\", (5,5)), (\"a\", (3,3)), (\"b\", (3,3)), (\"a\", 8)])\n",
      "print(x.subtract(y).collect())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('a', 2), ('c', 4), ('a', (1, 1))]\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = sc.parallelize(range(10))\n",
      "print(x.collect())\n",
      "print(x.keyBy(lambda x: x**2).collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "[(0, 0), (1, 1), (4, 2), (9, 3), (16, 4), (25, 5), (36, 6), (49, 7), (64, 8), (81, 9)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    }
   ],
   "metadata": {}
  }
 ]
}