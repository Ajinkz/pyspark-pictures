{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page1.svg\" width=500 height=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page2.svg\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click on an image to view the corresponding pyspark docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark version:1.6.1\n",
      "Ipython version:3.2.0\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "print(\"pyspark version:\" + str(sc.version))\n",
    "print(\"Ipython version:\" + str(IPython.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.agg\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page3.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------------------+\n",
      "|           avg(amt)|\n",
      "+-------------------+\n",
      "|0.20000000000000004|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.agg({\"amt\":\"avg\"})\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page4.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+\n",
      "|   to|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Carol|\n",
      "| Dave|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias\n",
    "from pyspark.sql.functions import col\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.alias('transactions')\n",
    "x.show()\n",
    "y.show()\n",
    "y.select(col(\"transactions.to\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page5.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# cache\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.cache()\n",
    "print(x.count()) # first action materializes x in memory\n",
    "print(x.count()) # later actions avoid IO overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page6.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# coalesce\n",
    "x_rdd = sc.parallelize([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)],2)\n",
    "x = sqlContext.createDataFrame(x_rdd, ['from','to','amt'])\n",
    "y = x.coalesce(numPartitions=1)\n",
    "print(x.rdd.getNumPartitions())\n",
    "print(y.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page7.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.collect()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page8.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "['from', 'to', 'amt']\n"
     ]
    }
   ],
   "source": [
    "# columns\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.columns\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page9.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "| from|   to|amt|  fee|\n",
      "+-----+-----+---+-----+\n",
      "|Alice|  Bob|0.1|0.001|\n",
      "|  Bob|Carol|0.2| 0.02|\n",
      "|Carol| Dave|0.3| 0.02|\n",
      "+-----+-----+---+-----+\n",
      "\n",
      "0.866025403784\n"
     ]
    }
   ],
   "source": [
    "# corr\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.corr(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page10.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "print(x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page11.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "| from|   to|amt|  fee|\n",
      "+-----+-----+---+-----+\n",
      "|Alice|  Bob|0.1|0.001|\n",
      "|  Bob|Carol|0.2| 0.02|\n",
      "|Carol| Dave|0.3| 0.02|\n",
      "+-----+-----+---+-----+\n",
      "\n",
      "0.00095\n"
     ]
    }
   ],
   "source": [
    "# cov\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.cov(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page12.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+----+-----+---+\n",
      "|from_to|Dave|Carol|Bob|\n",
      "+-------+----+-----+---+\n",
      "|    Bob|   0|    1|  0|\n",
      "|  Alice|   0|    0|  1|\n",
      "|  Carol|   1|    0|  0|\n",
      "+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crosstab\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.crosstab(col1='from',col2='to')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page13.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "<pyspark.sql.group.GroupedData object at 0x7f7a6341ed50>\n",
      "+-----+-----+-------------------+\n",
      "| from|   to|           sum(amt)|\n",
      "+-----+-----+-------------------+\n",
      "|Alice|Carol|                0.2|\n",
      "|Alice|  Bob|                0.1|\n",
      "|Alice| null|0.30000000000000004|\n",
      "| null|Carol|                0.2|\n",
      "| null|  Bob|                0.1|\n",
      "| null| null|0.30000000000000004|\n",
      "+-----+-----+-------------------+\n",
      "\n",
      "+-----+-----+--------+\n",
      "| from|   to|max(amt)|\n",
      "+-----+-----+--------+\n",
      "|Alice|Carol|     0.2|\n",
      "|Alice|  Bob|     0.1|\n",
      "|Alice| null|     0.2|\n",
      "| null|Carol|     0.2|\n",
      "| null|  Bob|     0.1|\n",
      "| null| null|     0.2|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Alice\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.cube('from','to')\n",
    "x.show()\n",
    "print(y) # y is a grouped data object, aggregations will be applied to all numerical columns\n",
    "y.sum().show() \n",
    "y.max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page14.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|                amt|\n",
      "+-------+-------------------+\n",
      "|  count|                  3|\n",
      "|   mean|0.20000000000000004|\n",
      "| stddev|0.09999999999999998|\n",
      "|    min|                0.1|\n",
      "|    max|                0.3|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page15.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "import numpy as np\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.distinct()\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page16.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "| from|   to|\n",
      "+-----+-----+\n",
      "|Alice|  Bob|\n",
      "|  Bob|Carol|\n",
      "|Carol| Dave|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.drop('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page17.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|  Bob|Carol|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropDuplicates / drop_duplicates\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Bob\",\"Carol\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropDuplicates(subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page18.svg\" width=500 height=500 />\n",
    "<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n",
      "+----+-----+----+\n",
      "|from|   to| amt|\n",
      "+----+-----+----+\n",
      "| Bob|Carol|null|\n",
      "| Bob|Carol| 0.2|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropna\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropna(how='any',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.pipe\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page19.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[('from', 'string'), ('to', 'string'), ('amt', 'double')]\n"
     ]
    }
   ],
   "source": [
    "# dtypes\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.dtypes\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page20.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [from#873,to#874,amt#875], MapPartitionsRDD[1295] at applySchemaToPythonRDD at null:-1\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "from: string, to: string, amt: double\n",
      "LogicalRDD [from#873,to#874,amt#875], MapPartitionsRDD[1295] at applySchemaToPythonRDD at null:-1\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [from#873,to#874,amt#875], MapPartitionsRDD[1295] at applySchemaToPythonRDD at null:-1\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan ExistingRDD[from#873,to#874,amt#875]\n"
     ]
    }
   ],
   "source": [
    "# explain\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.explain(extended = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreachPartition\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page21.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-------+-------+----+\n",
      "|   from|     to| amt|\n",
      "+-------+-------+----+\n",
      "|unknown|    Bob| 0.1|\n",
      "|    Bob|  Carol|null|\n",
      "|  Carol|unknown| 0.3|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fillna\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3)], ['from','to','amt'])\n",
    "y = x.fillna(value='unknown',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page22.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.filter(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page23.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "Row(from=u'Alice', to=u'Bob', amt=0.1)\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.first()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page24.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "PythonRDD[1329] at RDD at PythonRDD.scala:43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'Alice', 0.1, u'Bob', 0.2, u'Carol', 0.3]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.flatMap(lambda x: (x[0],x[2]))\n",
    "x.show()\n",
    "print(y)\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page25.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n",
      "None\n",
      "Row(from=u'Alice', to=u'Bob', amt=0.1)\n",
      "Row(from=u'Bob', to=u'Carol', amt=0.2)\n",
      "Row(from=u'Carol', to=u'Dave', amt=0.3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach\n",
    "from __future__ import print_function\n",
    "\n",
    "def fappend(el,f):\n",
    "    '''appends el to file f'''\n",
    "    print(el,file=open(f, 'a+') )\n",
    "\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "fn = './foreachExampleDataFrames.txt'\n",
    "open(fn, 'w').close()  # clear the file\n",
    "y = x.foreach(lambda x: fappend(x,fn)) # writes into foreachExampleDataFrames.txt\n",
    "\n",
    "print(x.collect()) # original dataframe\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of the file\n",
    "with open(fn, \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page26.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Carol', to=u'Dave', amt=0.3), Row(from=u'Bob', to=u'Carol', amt=0.2)]\n",
      "None\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n",
      "[Row(from=u'Bob', to=u'Carol', amt=0.2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreachPartition\n",
    "from __future__ import print_function\n",
    "\n",
    "def fappend(partition,f):\n",
    "    '''append all elements in partition to file f'''\n",
    "    print([el for el in partition],file=open(f, 'a+'))\n",
    "\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x = x.repartition(2) # force 2 partitions\n",
    "fn = './foreachExampleDataFrames.txt'\n",
    "open(fn, 'w').close()  # clear the file\n",
    "y = x.foreachPartition(lambda x: fappend(x,fn)) # writes into foreachExampleDataFrames.txt\n",
    "\n",
    "print(x.collect()) # original dataframe\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of the file\n",
    "with open(fn, \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page27.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.1|\n",
      "|Alice| Dave|0.1|\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|  Bob|0.5|\n",
      "|Carol|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+--------------+-------------+\n",
      "|from_freqItems|amt_freqItems|\n",
      "+--------------+-------------+\n",
      "|       [Alice]|        [0.1]|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# freqItems\n",
    "x = sqlContext.createDataFrame([('Bob',\"Carol\",0.1), \\\n",
    "                                (\"Alice\",\"Dave\",0.1), \\\n",
    "                                (\"Alice\",\"Bob\",0.1), \\\n",
    "                                (\"Alice\",\"Bob\",0.5), \\\n",
    "                                (\"Carol\",\"Bob\",0.1)], \\\n",
    "                               ['from','to','amt'])\n",
    "y = x.freqItems(cols=['from','amt'],support=0.8)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page28.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "<pyspark.sql.group.GroupedData object at 0x7f7a6334c490>\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.groupBy('from')\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page29.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-------------------+\n",
      "| from|           avg(amt)|\n",
      "+-----+-------------------+\n",
      "|Carol|                0.3|\n",
      "|Alice|0.15000000000000002|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy(col1).avg(col2)\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.groupBy('from').avg('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.histogram\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page30.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2)]\n"
     ]
    }
   ],
   "source": [
    "# head\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.head(2)\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page31.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Alice|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.1|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intersect\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Alice\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.intersect(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page32.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# isLocal\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.isLocal()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page33.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 20|\n",
      "|  Bob| 40|\n",
      "| Dave| 80|\n",
      "+-----+---+\n",
      "\n",
      "+-----+----+---+----+---+\n",
      "| from|  to|amt|name|age|\n",
      "+-----+----+---+----+---+\n",
      "|Carol|Dave|0.3|Dave| 80|\n",
      "|Alice| Bob|0.1| Bob| 40|\n",
      "+-----+----+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',20),(\"Bob\",40),(\"Dave\",80)], ['name','age'])\n",
    "z = x.join(y,x.to == y.name,'inner')\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleStdev\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page34.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.limit(2)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleVariance\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page35.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[1.1, 1.2, 1.3]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.map(lambda x: x.amt+1)\n",
    "x.show()\n",
    "print(y.collect())  # output is a rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page36.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "[[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Carol', to=u'Dave', amt=0.3)], [Row(from=u'Bob', to=u'Carol', amt=0.2)]]\n",
      "[0.4, 0.2]\n",
      "[[0.4], [0.2]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions\n",
    "def amtsum(partition):\n",
    "    '''sum the value in field amt'''\n",
    "    yield sum([el.amt for el in partition])\n",
    "    \n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x = x.repartition(2)\n",
    "y = x.mapPartitions(lambda p: amtsum(p))\n",
    "x.show()\n",
    "print(x.rdd.glom().collect()) # flatten elements on the same partition\n",
    "print(y.collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page37.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n",
      "<pyspark.sql.dataframe.DataFrameNaFunctions object at 0x7f7a6334cfd0>\n",
      "+----+-----+---+\n",
      "|from|   to|amt|\n",
      "+----+-----+---+\n",
      "| Bob|Carol|0.2|\n",
      "+----+-----+---+\n",
      "\n",
      "+-------+-------+---+\n",
      "|   from|     to|amt|\n",
      "+-------+-------+---+\n",
      "|unknown|    Bob|0.1|\n",
      "|    Bob|  Carol|0.0|\n",
      "|  Carol|unknown|0.3|\n",
      "|    Bob|  Carol|0.2|\n",
      "+-------+-------+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "| null|  Bob|0.1|\n",
      "|  Bob|Carol|0.0|\n",
      "|Carol| null|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# na\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.na  # returns an object for handling missing values, supports drop, fill, and replace methods\n",
    "x.show()\n",
    "print(y)\n",
    "y.drop().show()\n",
    "y.fill({'from':'unknown','to':'unknown','amt':0}).show()\n",
    "y.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page38.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.orderBy(['from'],ascending=[False])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page39.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.persist(storageLevel=StorageLevel(True,True,False,True,1)) # StorageLevel(useDisk,useMemory,useOffHeap,deserialized,replication=1)\n",
    "x.show()\n",
    "x.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page40.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page41.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Alice| Bob|0.1|\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n",
      "+----+-----+---+\n",
      "|from|   to|amt|\n",
      "+----+-----+---+\n",
      "| Bob|Carol|0.2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomSplit\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.randomSplit([0.5,0.5])\n",
    "x.show()\n",
    "y[0].show()\n",
    "y[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page42.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# rdd\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rdd\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page43.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registerTempTable\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.registerTempTable(name=\"TRANSACTIONS\")\n",
    "y = sqlContext.sql('SELECT * FROM TRANSACTIONS WHERE amt > 0.1')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page44.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# repartition\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.repartition(3)\n",
    "print(x.rdd.getNumPartitions())\n",
    "print(y.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKeyLocally\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page45.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|David|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.replace('Dave','David',['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page46.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+------------------+\n",
      "| from|   to|          sum(amt)|\n",
      "+-----+-----+------------------+\n",
      "|Alice|  Bob|               0.1|\n",
      "|  Bob|Carol|               0.2|\n",
      "|Alice| null|               0.1|\n",
      "|Carol| Dave|               0.3|\n",
      "|  Bob| null|               0.2|\n",
      "|Carol| null|               0.3|\n",
      "| null| null|0.6000000000000001|\n",
      "+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rollup\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rollup(['from','to'])\n",
    "x.show()\n",
    "y.sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page47.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.sample(False,0.5)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page48.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|Carol|0.2|\n",
      "|Alice|Alice|0.3|\n",
      "|Alice| Dave|0.4|\n",
      "|  Bob|  Bob|0.5|\n",
      "|  Bob|Carol|0.6|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|  Bob|0.5|\n",
      "|  Bob|Carol|0.6|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sampleBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Alice\",\"Alice\",0.3), \\\n",
    "                               ('Alice',\"Dave\",0.4),(\"Bob\",\"Bob\",0.5),(\"Bob\",\"Carol\",0.6)], \\\n",
    "                                ['from','to','amt'])\n",
    "y = x.sampleBy(col='from',fractions={'Alice':0.1,'Bob':1.0})\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page49.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "StructType(List(StructField(from,StringType,true),StructField(to,StringType,true),StructField(amt,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "# schema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.schema\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.partitionBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page50.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| from|amt|\n",
      "+-----+---+\n",
      "|Alice|0.1|\n",
      "|  Bob|0.2|\n",
      "|Carol|0.3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.select(['from','amt'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.combineByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page51.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+----------------+----------+\n",
      "|substr(from,1,1)|(amt + 10)|\n",
      "+----------------+----------+\n",
      "|               A|      10.1|\n",
      "|               B|      10.2|\n",
      "|               C|      10.3|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.selectExpr(['substr(from,1,1)','amt+10'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page52.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page53.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|Alice|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol|Alice|0.3|\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Alice\",0.3)], ['from','to','amt'])\n",
    "y = x.sort(['to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page54.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+----+\n",
      "| from|   to|amt|p_id|\n",
      "+-----+-----+---+----+\n",
      "|Alice|  Bob|0.1|   1|\n",
      "|  Bob|Carol|0.2|   2|\n",
      "|Carol|Alice|0.3|   2|\n",
      "+-----+-----+---+----+\n",
      "\n",
      "+-----+-----+---+----+\n",
      "| from|   to|amt|p_id|\n",
      "+-----+-----+---+----+\n",
      "|Alice|  Bob|0.1|   1|\n",
      "|Carol|Alice|0.3|   2|\n",
      "|  Bob|Carol|0.2|   2|\n",
      "+-----+-----+---+----+\n",
      "\n",
      "[[Row(from=u'Alice', to=u'Bob', amt=0.1, p_id=1)], [Row(from=u'Bob', to=u'Carol', amt=0.2, p_id=2), Row(from=u'Carol', to=u'Alice', amt=0.3, p_id=2)]]\n",
      "[[Row(from=u'Alice', to=u'Bob', amt=0.1, p_id=1)], [Row(from=u'Carol', to=u'Alice', amt=0.3, p_id=2), Row(from=u'Bob', to=u'Carol', amt=0.2, p_id=2)]]\n"
     ]
    }
   ],
   "source": [
    "# sortWithinPartitions\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1,1),(\"Bob\",\"Carol\",0.2,2),(\"Carol\",\"Alice\",0.3,2)], \\\n",
    "                               ['from','to','amt','p_id']).repartition(2,'p_id')\n",
    "y = x.sortWithinPartitions(['to'])\n",
    "x.show()\n",
    "y.show()\n",
    "print(x.rdd.glom().collect()) # glom() flattens elements on the same partition\n",
    "print(y.rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page56.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "| from|   to|amt|  fee|\n",
      "+-----+-----+---+-----+\n",
      "|Alice|  Bob|0.1|0.001|\n",
      "|  Bob|Carol|0.2| 0.02|\n",
      "|Carol| Dave|0.3| 0.02|\n",
      "+-----+-----+---+-----+\n",
      "\n",
      "0.866025403784\n"
     ]
    }
   ],
   "source": [
    "# stat\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.stat.corr(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupWith\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page57.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.subtract(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cogroup\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page58.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2)]\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.take(num=2)\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page59.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+------+-----+---+\n",
      "|seller|buyer|amt|\n",
      "+------+-----+---+\n",
      "| Alice|  Bob|0.1|\n",
      "|   Bob|Carol|0.2|\n",
      "| Carol| Dave|0.3|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toDF(\"seller\",\"buyer\",\"amt\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtractByKey\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page60.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[u'{\"from\":\"Alice\",\"to\":\"Bob\",\"amt\":0.1}', u'{\"from\":\"Bob\",\"to\":\"Carol\",\"amt\":0.2}', u'{\"from\":\"Carol\",\"to\":\"Dave\",\"amt\":0.3}']\n"
     ]
    }
   ],
   "source": [
    "# toJSON\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toJSON()\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page61.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> Alice</td>\n",
       "      <td>   Bob</td>\n",
       "      <td> 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>   Bob</td>\n",
       "      <td> Carol</td>\n",
       "      <td> 0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> Carol</td>\n",
       "      <td>  Dave</td>\n",
       "      <td> 0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    from     to  amt\n",
       "0  Alice    Bob  0.1\n",
       "1    Bob  Carol  0.2\n",
       "2  Carol   Dave  0.3\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toPandas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toPandas()\n",
    "x.show()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keyBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page62.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionAll\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.unionAll(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page63.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpersist\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.cache()\n",
    "x.count()\n",
    "x.unpersist()\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page64.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where (filter)\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.filter(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page65.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+----+\n",
      "| from|   to|amt|conf|\n",
      "+-----+-----+---+----+\n",
      "|Alice|  Bob|0.1|true|\n",
      "|  Bob|Carol|0.2|true|\n",
      "|Carol| Dave|0.3|true|\n",
      "+-----+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumn('conf',x.amt.isNotNull())\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page66.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+------+\n",
      "| from|   to|amount|\n",
      "+-----+-----+------+\n",
      "|Alice|  Bob|   0.1|\n",
      "|  Bob|Carol|   0.2|\n",
      "|Carol| Dave|   0.3|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumnRenamed('amt','amount')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page67.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+---+-----+-----+\n",
      "|amt| from|   to|\n",
      "+---+-----+-----+\n",
      "|0.1|Alice|  Bob|\n",
      "|0.2|  Bob|Carol|\n",
      "|0.3|Carol| Dave|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write\n",
    "import json\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.write.json('./dataframeWriteExample.json',mode='overwrite')\n",
    "x.show()\n",
    "# read the dataframe back in from file\n",
    "sqlContext.read.json('./dataframeWriteExample.json').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
